# AI/ML Interview Preparation: Basic Notes & Q&A
**Focus:** Core concepts, algorithms, math, and practical applications.

---

## I. Basic Notes

### Machine Learning Fundamentals
- **Types**:
  - **Supervised**: Labeled data (e.g., classification, regression)
  - **Unsupervised**: Unlabeled data (e.g., clustering, dimensionality reduction)
  - **Reinforcement**: Reward-based learning (e.g., game AI, robotics)
  
- **Key Concepts**:
  - **Overfitting**: Model fits training noise ‚Üí fails on new data  
    *Fix*: Regularization, cross-validation, more data
  - **Underfitting**: Model too simple ‚Üí misses patterns  
    *Fix*: Increase complexity, feature engineering
  - **Bias-Variance Tradeoff**:
    - *Bias*: Oversimplification error (high bias ‚Üí underfitting)
    - *Variance*: Noise sensitivity error (high variance ‚Üí overfitting)
    - *Goal*: Balance both
  - **Regularization**:
    - **L1 (Lasso)**: `ŒªŒ£|weights|` ‚Üí sparse features
    - **L2 (Ridge)**: `ŒªŒ£weights¬≤` ‚Üí prevents large weights
  - **Evaluation Metrics**:
    - **Classification**: Accuracy, Precision, Recall, F1, ROC-AUC
    - **Regression**: MSE, MAE, R¬≤

### Core Algorithms
| Algorithm | Type | Key Idea | Use Case |
|-----------|------|----------|----------|
| **Linear Regression** | Supervised | Linear feature combination | House prices |
| **Logistic Regression** | Supervised | Probability prediction (sigmoid) | Spam detection |
| **Decision Trees** | Supervised | Feature-based splits | Customer churn |
| **Random Forests** | Supervised | Ensemble of trees (bagging) | Credit scoring |
| **SVM** | Supervised | Optimal hyperplane + kernels | Image classification |
| **K-Means** | Unsupervised | Minimize within-cluster variance | Customer segmentation |
| **Neural Networks** | Both | Layered weight optimization | Complex patterns |

### Deep Learning
- **Activation Functions**:
  - **ReLU**: `max(0, x)` (fast, avoids vanishing gradient)
  - **Sigmoid**: `1/(1+e‚ÅªÀ£)` (probability output)
  - **Softmax**: Multi-class probability distribution
  
- **Architectures**:
  - **CNN**: Convolution (feature extraction) ‚Üí Pooling (downsampling) ‚Üí FC (classification)  
    *Use*: Image recognition
  - **RNN/LSTM**: Recurrent connections + memory cells (LSTM/GRU)  
    *Use*: Time-series, NLP
  - **Transformers**: Self-attention + positional encoding  
    *Use*: NLP (BERT, GPT)

### NLP Essentials
- **Text Preprocessing**:  
  Tokenization ‚Üí Stemming/Lemmatization ‚Üí Stop-word removal
- **Embeddings**:
  - **Word2Vec/GloVe**: Static word vectors
  - **BERT**: Contextual embeddings
- **Tasks**: Sentiment analysis, NER, Machine Translation

### Mathematics
- **Linear Algebra**: Vectors, matrices, eigenvalues (PCA)
- **Calculus**: Derivatives, gradients, chain rule (backpropagation)
- **Probability**: Distributions (Gaussian, Bernoulli), Bayes' theorem

### Practical Skills
- **Data Prep**: Handle missing data, normalization (StandardScaler/MinMaxScaler)
- **Validation**: k-fold cross-validation
- **Tuning**: Grid search, random search, Bayesian optimization
- **Deployment**: Flask/FastAPI APIs, cloud platforms (AWS SageMaker)

---

## II. Must-Know Q&A

### Q1: Explain overfitting and prevention
**Answer**:  
Overfitting = model learns training noise ‚Üí poor generalization  
**Prevention**:  
- Regularization (L1/L2)  
- Cross-validation  
- Dropout (NN)  
- Early stopping  
- More data  

### Q2: Bias-variance tradeoff?
**Answer**:  
- **Bias**: Error from oversimplification (underfitting)  
- **Variance**: Error from noise sensitivity (overfitting)  
- **Tradeoff**: ‚Üë complexity ‚Üí ‚Üì bias but ‚Üë variance. Goal: Minimize total error.  

### Q3: How CNNs work
**Answer**:  
1. **Convolution**: Apply filters to detect features (edges/textures)  
2. **Pooling**: Reduce dimensions (e.g., max pooling)  
3. **Fully Connected**: Classify using extracted features  
**Key**: Parameter sharing + spatial hierarchy  

### Q4: Backpropagation explained
**Answer**:  
NN training algorithm:  
1. Forward pass ‚Üí predictions  
2. Compute loss (e.g., cross-entropy)  
3. Backward pass ‚Üí calculate gradients (chain rule)  
4. Update weights via gradient descent  

### Q5: L1 vs L2 regularization
**Answer**:  
- **L1 (Lasso)**: `ŒªŒ£|weights|` ‚Üí sparsity (some weights ‚Üí 0)  
- **L2 (Ridge)**: `ŒªŒ£weights¬≤` ‚Üí small weights (keeps all features)  

### Q6: Why activation functions?
**Answer**:  
Introduce non-linearity ‚Üí enables learning complex patterns. Without them, NNs become linear models.  

### Q7: How LSTMs solve vanishing gradients
**Answer**:  
Memory cells + gates (input/forget/output) regulate information flow. Cell state preserves gradients over long sequences.  

### Q8: BoW vs TF-IDF
**Answer**:  
- **BoW**: Raw word counts (ignores order)  
- **TF-IDF**: Word importance weighting:  
  - **TF**: Frequency in document  
  - **IDF**: Rarity across documents  

### Q9: PCA explained
**Answer**:  
Dimensionality reduction:  
1. Standardize data  
2. Compute covariance matrix  
3. Eigen decomposition ‚Üí eigenvectors (principal components)  
4. Project data onto top *k* components  

### Q10: Handling imbalanced data
**Answer**:  
- **Resampling**: Oversample minority (SMOTE) or undersample majority  
- **Class weights**: Penalize minority misclassification  
- **Metrics**: Use F1/precision/recall (not accuracy)  

### Q11: Generative vs Discriminative models
**Answer**:  
- **Generative**: Learn `P(X,Y)` ‚Üí can generate data (e.g., GANs, Naive Bayes)  
- **Discriminative**: Learn `P(Y|X)` ‚Üí focus on decision boundaries (e.g., SVM, logistic regression)  

### Q12: Transformer architecture
**Answer**:  
- **Self-Attention**: Word-to-word relevance scoring  
- **Positional Encoding**: Injects word order info  
- **Encoder-Decoder**: Input processing ‚Üí output generation  
- **Parallelization**: Processes all tokens simultaneously  

### Q13: Type I vs Type II errors
**Answer**:  
- **Type I (False Positive)**: Incorrectly reject true null (e.g., "spam" when not)  
- **Type II (False Negative)**: Fail to reject false null (e.g., "not spam" when it is)  

### Q14: Gradient descent
**Answer**:  
Optimization algorithm:  
1. Initialize random weights  
2. Compute loss gradient  
3. Update: `weights = weights - learning_rate * gradient`  
**Variants**: SGD, Adam  

### Q15: Transfer learning
**Answer**:  
Reuse pre-trained models (e.g., ResNet, BERT) on new tasks via fine-tuning. Saves resources and improves performance with limited data.  

---

## III. Pro Tips for the Interview
1. **Revise Projects**:  
   - Explain your role, approach, and results  
   - Quantify impact (e.g., "Improved accuracy by 15%")  

2. **Whiteboard Practice**:  
   - Implement algorithms from scratch (e.g., k-NN, decision trees)  
   - Solve problems on LeetCode/HackerRank  

3. **Conceptual Clarity**:  
   - Explain ML simply (e.g., "Like teaching a child to recognize cats with pictures")  

4. **Behavioral Prep**:  
   - "Describe a project challenge"  
   - "How do you handle conflicting priorities?"  

5. **Company Research**:  
   - Study their tech stack (TensorFlow/PyTorch?)  
   - Understand their ML applications  

6. **Ask Smart Questions**:  
   - "How does your team deploy models?"  
   - "What's your biggest ML challenge?"  

**Final Advice**: Focus on fundamentals, think aloud, and stay calm! üöÄ